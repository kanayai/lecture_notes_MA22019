---
title: "Instructor Solution Key & Assessment Guide"
format:
  html:
    toc: true
    code-fold: show
---

This page contains the answers to all quizzes, homework assignments, and advanced expansion challenges for the 10-hour Text Analysis course.

---

## Module 1: Foundations & Tokenization

### Part A: Concept Quiz

**Q1. True or False:** The "Tidy Text" format is structured as one-document-per-row.
> **Answer:** **False.** The Tidy Text principle is **one-token-per-row**. This structure allows us to use standard data manipulation tools like `dplyr` to filter and join datasets at the word level.

**Q2. Multiple Choice:** Which specific R function is used to split a text column into individual tokens?
* A) `separate_rows()`
* B) `unnest_tokens()`
* C) `str_split()`
> **Answer:** **B) `unnest_tokens()`**. While `str_split` can do this, `unnest_tokens` is the dedicated tidytext function that handles lowercasing and punctuation removal automatically.

**Q3. True/False:** Tokenization always splits text strictly by white space.
> **Answer:** **False.** Good tokenizers (like those in `tidytext` or `nltk`) handle punctuation intelligently. For example, "It's" might be split into "It" and "'s" depending on the engine, and "New York" might be kept as one token if n-grams are used.

### Part B: Practical Homework (The Time Machine)

**Task:** Download H.G. Wells' *The Time Machine* and perform tokenization.

**R Solution:**
```{r}
#| eval: false
library(gutenbergr)
library(tidytext)
library(dplyr)

# ID 35 is The Time Machine
hg_wells <- gutenberg_download(35) %>% 
  unnest_tokens(word, text)

# Inspect the tidy structure
head(hg_wells)
```

**Task:** Find the top 10 most common **trigrams**.

**R Solution:**
```{r}
#| eval: false
hg_wells_trigrams <- gutenberg_download(35) %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  count(trigram, sort = TRUE)

head(hg_wells_trigrams, 10)
```


### Part C: Advanced Expansion Challenge

**Task:** Remove trigrams that contain stop words.

**R Solution:**
```{r}
#| eval: false
library(tidyr)
data("stop_words")

hg_wells_trigrams %>%
  separate(trigram, c("w1", "w2", "w3"), sep = " ") %>%
  filter(!w1 %in% stop_words$word,
         !w2 %in% stop_words$word,
         !w3 %in% stop_words$word) %>%
  unite(trigram, w1, w2, w3, sep = " ") %>%
  count(trigram, sort = TRUE)
```

## Module 2: Preprocessing

### Part A: Concept Quiz

Q1. Concept Check: What is the primary difference between Stemming and Lemmatization?

Answer: > * Stemming is a heuristic process that chops off the ends of words (e.g., "ponies" -> "poni"). It is fast but crude.
> * Lemmatization uses a dictionary and morphological analysis to find the actual root form (e.g., "ponies" -> "pony").

Q2. Concept Check: In the TF-IDF formula, what is the purpose of the IDF (Inverse Document Frequency) component?

Answer: To penalize words that appear in every document. If a word like "the" appears in all documents, its IDF score is near zero, lowering its overall weight.

Q3. R Knowledge: Which function converts a tidy data frame into a sparse matrix?

A) cast_sparse() or cast_dtm()

B) as.matrix()

Answer: A) cast_dtm() (or cast_sparse). This is crucial for connecting tidytext workflows to machine learning packages like topicmodels.

### Part B: Advanced Practice

**R Challenge:** Visualize the top 10 words in Jane Austen's Persuasion.

```{r}
#| eval: false
library(ggplot2)

tidy_books %>%
  filter(book == "Persuasion") %>%
  count(word, sort = TRUE) %>%
  top_n(10, n) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Top Words in Persuasion")
```

## Module 3: Topic Modeling

### Part A: Concept Quiz

Q1. True/False: LDA is a supervised learning algorithm.

Answer: False. LDA is unsupervised. We do not tell it what the topics are in advance.

Q2. Parameter Check: What controls the number of topics the model finds?

A) Alpha
B) Beta
C) k

Answer: C) k. You must specify k (e.g., k=2 or k=5) when initializing the model.

### Part B: Practical Homework (Interpreting Topics)

Task: Run LDA with k=3 and interpret the results.

R Solution:

```{r}
#| eval: false
# Fit model with 3 topics
lda_3 <- LDA(dtm, k = 3, control = list(seed = 1234))

# Extract words
terms(lda_3, 5)
```
Interpretation Example:
Topic 1: "love, heart, dear" -> Label: Romance
Topic 2: "money, pounds, estate" -> Label: Wealth
Topic 3: "time, house, day" -> Label: Daily Life

### Part C: Advanced Expansion Challenge

**R Challenge:** Inspect the Gamma (Î³) matrix to see document probabilities.

```{r}
#| eval: false
# The 'gamma' matrix shows the probability that a specific document belongs to a topic
gamma_results <- tidy(lda_model, matrix = "gamma")

# Sort to find which document is most strongly associated with Topic 1
gamma_results %>% 
  filter(topic == 1) %>%
  arrange(desc(gamma))
```

## Module 4: Applications & Inference

### Part A: Concept Quiz

Q1. Score Range: The `bing` lexicon classifies words into:
A) Positive / Negative
B) -5 to +5 
C) Emotions (Joy, Anger, etc.)

Answer: A) Positive / Negative. (B is AFINN, C is NRC).

### Part B: Practical Challenge (Sentiment Analysis)

**Task:** Count positive/negative words in *Emma*.

**R Solution:**

```{r}
#| eval: false
emma_sentiment <- tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(get_sentiments("bing")) %>%
  count(sentiment)

print(emma_sentiment)
# Expected: Usually slightly more positive words than negative in Austen
```