---
title: "Module 2: Preprocessing & Feature Engineering"
---

## Theory (15 Minutes)

### Why Clean Data? (Normalization)
Computers are dumb. To a computer, "Apple", "apple", and "apples" are three completely different things. **Normalization** is the process of mapping these variations to a single canonical form to reduce the "vocabulary size" (dimensionality) of our data.

### Key Techniques

1.  **Stop Words**: 
    Words like "the", "is", "at" appear frequently but often carry little semantic meaning for topics. Removing them reduces noise.
    *   *Warning*: Be careful! "To be or not to be" is entirely stop words. Context matters.

2.  **Stemming vs. Lemmatization**:

    *   **Stemming**: A crude heuristic that chops off ends of words. Fast, but dumb.
        *   *Example*: "Organization" -> "Organ". (Now your business document sounds like a medical one!)
        *   *Example*: "Universities" -> "Univers".
    *   **Lemmatization**: Uses a dictionary (lexicon) to return the base root form (lemma) of a word.
        *   *Example*: "Better" -> "Good". (Stemming would fail here).

3.  **TF-IDF (Term Frequency-Inverse Document Frequency)**:
    A statistical metric to evaluate how important a word is to a document in a collection.
    
    $$ \text{TF-IDF} = \text{TF}(w,d) \times \log\left(\frac{N}{\text{df}(w)}\right) $$

    *   **The Log Logic**: Why do we take the log of the inverse frequency?
        *   If a word appears in *all* documents (e.g., "the"), $\frac{N}{\text{df}} = 1$ and $\log(1) = 0$. The word effectively vanishes.
        *   The log dampens the effect of huge rarity differences, creating a smooth scale of importance.

---

## Practical (135 Minutes)

### R Exercise 2.1: Cleaning and TF-IDF

We will inspect our most common "noise" words and then down-weight them using TF-IDF.

```{r}
#| label: r-preprocessing
library(tidytext)
library(dplyr)
library(ggplot2)

# 1. Load the tidy data saved in Module 1
tidy_books <- readRDS(here::here("data", "tidy_books.rds"))

# 2. Inspect Stop Words
data("stop_words") # Provided by tidytext
head(stop_words)

# 3. Remove Stop Words (Anti-Join)
# Think of this as: "Keep rows in A that do NOT match rows in B"
tidy_books_clean <- tidy_books %>%
    anti_join(stop_words, by = "word")

# 4. Calculate TF-IDF
# We want to know: What words are unique to each book?
book_tf_idf <- tidy_books_clean %>%
    count(book, word, sort = TRUE) %>%
    bind_tf_idf(word, book, n)

# 5. Visualize Top Words per Book
book_tf_idf %>%
    group_by(book) %>%
    slice_max(tf_idf, n = 5) %>%
    ungroup() %>%
    mutate(word = reorder_within(word, tf_idf, book)) %>%
    ggplot(aes(tf_idf, word, fill = book)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~book, scales = "free") +
    scale_y_reordered() +
    labs(title = "Highest TF-IDF Words by Book", x = "TF-IDF Score", y = NULL)
```

### R Exercise 2.2: Lemmatization & Bag-of-Words

Standard `tidytext` tokenization often leaves words in their plural forms or different conjugations (e.g., "run", "running", "ran"). **Lemmatization** reduces these to their root form.

We will use the `textstem` package, which is excellent for lemmatizing vector strings in R.

```{r}
#| label: r-lemmatization
library(textstem)
library(tidyr)

# 1. Inspect words before lemmatization
# Let's look at forms of the word "love" in Jane Austen
tidy_books %>%
    filter(grepl("^lov", word)) %>%
    count(word, sort = TRUE) %>%
    head(5)

# 2. Apply Lemmatization
# We mutate the 'word' column to its lemma
tidy_books_lemm <- tidy_books %>%
    mutate(lemma = lemmatize_words(word))

# 3. Inspect results
# "loved", "loves", "loving" should all map to "love"
tidy_books_lemm %>%
    filter(grepl("^lov", lemma)) %>%
    count(lemma, sort = TRUE) %>%
    head(5)
```

### Manual "Bag of Words" (DTM) Construction

Machine learning algorithms (like Topic Modeling) generally don't accept a tidy data frame. They need a matrix where rows are documents and columns are words. This is often called a **Document-Term Matrix (DTM)**.

```{r}
#| label: r-dtm-creation

# 1. Create a count table
# We count how many times each word appears in each book
word_counts <- tidy_books_lemm %>%
    anti_join(stop_words, by = "word") %>%
    count(book, lemma, sort = TRUE)

# 2. Cast to DTM
# This converts the long tidy format into a sparse matrix
dtm_books <- word_counts %>%
    cast_dtm(book, lemma, n)

# Inspect the DTM
dtm_books
# Note the Sparsity: content is mostly empty (0s) because most words don't appear in most documents.
```

---

## Concept Quiz

1.  **True or False:** In the "tidy text" format, the standard structure is one-document-per-row.
    *   *Answer: False (one-token-per-row)*
2.  **Short Answer:** Briefly explain the difference between **Stemming** and **Lemmatization**.
    *   *Answer: Stemming chops off suffixes (fast, crude), Lemmatization finds the dictionary root (slower, accurate).*

*Check your work against the Solution Key.*