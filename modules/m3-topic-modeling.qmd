---
title: "Module 3: Topic Modeling"
---

## Theory (20 Minutes)

### Latent Dirichlet Allocation (LDA)

LDA is an unsupervised machine learning algorithm that discovers hidden "topics" in a collection of documents.

**The Intuition: Food vs. Pets**
Imagine you have 3 documents:

1.  "The cat ate the tuna."
2.  "The dog chased the cat."
3.  "The tuna was cooked."

And 2 hidden topics: **A (Pets)** and **B (Food)**.

*   The word "cat" would have a high probability in Topic A (Pets).
*   The word "tuna" would have a high probability in Topic B (Food).
*   Document 1 ("cat ate tuna") is a **mixture**: 50% Topic A + 50% Topic B.

LDA reverse-engineers this. It asks: *"Given these words, what mix of topics best explains them?"*

**The Hyperparameters (The Knobs):**

*   **Alpha ($\alpha$):** Are documents focused on just 1 topic (Low Alpha) or do they cover many topics (High Alpha)?
*   **Beta/Eta ($\eta$):** Are topics made of just a few specific words (Low Beta) or many broad words (High Beta)?

**The Math (Simplified):**

*   $\beta$ (Beta): The probability of a word given a topic.
*   $\gamma$ (Gamma): The probability of a topic given a document.

We reverse-engineer these distributions from the observed documents.

---

## Practical (130 Minutes)

### R Exercise 3.1: Running LDA with `topicmodels`

We will use the DTM created in the previous steps to run LDA.

```{r}
#| label: r-lda
#| message: false
#| warning: false
library(topicmodels)
library(tidytext)
library(dplyr)
library(ggplot2)
library(janeaustenr)

# 1. Setup Data (Recap from Module 2)
# We need a Document-Term Matrix (DTM) for the topicmodels package
data("stop_words")
dtm <- austen_books() %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words, by = "word") %>%
    count(book, word) %>%
    cast_dtm(book, word, n)

# 2. Run LDA
# k=2: We ask the model to find 2 distinct topics
# seed: Ensures reproducibility
lda_model <- LDA(dtm, k = 2, control = list(seed = 1234))

# 3. Inspect Word-Topic Probabilities (Beta)
# tidy() converts the complex model object back into a data frame
topics <- tidy(lda_model, matrix = "beta")

# 4. Visualize Top Terms per Topic
# This answers: "What words define Topic 1 vs Topic 2?"
top_terms <- topics %>%
    group_by(topic) %>%
    slice_max(beta, n = 10) %>%
    ungroup() %>%
    arrange(topic, -beta)

top_terms %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~topic, scales = "free") +
    scale_y_reordered() +
    labs(title = "Top Words per Topic", x = "Beta (Probability)")
```

### R Exercise 3.2: From Raw Strings to LDA

Sometimes you don't start with a book corpus but with a simple list of strings (e.g., survey responses, tweets). Here is how you go from a simple vector to a topic model in R.

```{r}
#| label: r-raw-lda
# 1. Sample Data (Vector of strings)
raw_documents <- c(
    "Machine learning uses statistics to find patterns in data.",
    "Natural language processing analyzes text data structure.",
    "Deep learning requires vast amounts of data and gpu power.",
    "Politics and elections are dominated by voting and polls.",
    "The senate voted on the new bill yesterday."
)

# 2. Create a Tibble
# We need a dataframe structure to use Tidytext
text_df <- tibble(
    doc_id = 1:5,
    text = raw_documents
)

# 3. Process to DTM
# Pipeline: Tokenize -> Remove Stopwords -> Count -> Cast DTM
dtm_small <- text_df %>%
    unnest_tokens(word, text) %>%
    anti_join(stop_words, by = "word") %>%
    count(doc_id, word) %>%
    cast_dtm(doc_id, word, n)

# 4. Train LDA Model
# We ask for k=2 topics (expecting "Tech" vs "Politics")
lda_small <- LDA(dtm_small, k = 2, control = list(seed = 42))

# 5. Interpret Topics
# We look at the top beta probabilities
tidy(lda_small, matrix = "beta") %>%
    group_by(topic) %>%
    slice_max(beta, n = 5) %>%
    arrange(topic, -beta) %>%
    ggplot(aes(beta, reorder_within(term, beta, topic), fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~topic, scales = "free") +
    scale_y_reordered() +
    labs(title = "Discovered Topics (Small Example)", y = NULL)
```

---

## Homework: Interpreting Topics

**Task:**
1.  Run the LDA code from the practical section with `k=3` topics instead of 2.
2.  Look at the top words for each of the 3 topics.
3.  **Concept Question:** Based on the top words, give a human-readable label to each topic (e.g., "Romance", "Daily Life", "Politics").

*Check your work against the Solution Key.*