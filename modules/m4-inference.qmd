---
title: "Module 4: Applications & Inference"
---

## Theory (15 Minutes)

### Beyond Counting: Inference

Once we have numbers (tokens, frequencies, topics), we can start asking questions.

1.  **Sentiment Analysis (VADER)**:
    VADER (Valence Aware Dictionary and sEntiment Reasoner) is designed for social media. It doesn't just count positive words. It uses **heuristics**:
    
    *   **Punctuation**: "Good!!!" > "Good".
    *   **Capitalization**: "GOOD" > "good".
    *   **Negation**: "Not good" flips the score.
    *   **Degree Modifiers**: "Extremely good" > "good".

2.  **Comparative Analysis (Log-Odds)**:
    When comparing vocabulary between two groups (e.g., Jane Austen vs. H.G. Wells), we often plot them on a logarithmic scale.
    *   **Why Log?** Zipf's Law states that a few words are extremely common ("the", "and") while most are rare. A linear scale squashes all the interesting words into the corner. A Log-Log plot spreads them out so we can see the differences.

3.  **Named Entity Recognition (NER)**:
    Identifying real-world objects like "Apple" (Organization) vs. "apple" (fruit).

---

## Practical (135 Minutes)

### R Exercise 4.1: Comparative Analysis

We will compare two of Jane Austen's books to see if her vocabulary shifted.

```{r}
#| label: r-compare
#| message: false
#| warning: false
library(tidyr)
library(stringr)
library(dplyr)
library(janeaustenr)
library(tidytext)
library(ggplot2)

# 1. Setup Data: Comparing "Emma" vs "Persuasion"
books <- austen_books() %>%
    filter(book %in% c("Emma", "Persuasion"))

# 2. Calculate Word Frequencies by Book
frequency <- books %>%
    unnest_tokens(word, text) %>%
    # Keep only letters (remove numbers/punctuation artifacts)
    filter(str_detect(word, "[a-z]+")) %>%
    count(book, word) %>%
    group_by(book) %>%
    # Normalize counts to frequencies (because book lengths differ)
    mutate(proportion = n / sum(n)) %>%
    select(-n) %>%
    pivot_wider(names_from = book, values_from = proportion) %>%
    # Pivot wider introduces NAs for words present in one book but not the other
    # We use pivot_longer/gather logic or simple plotting
    pivot_longer(cols = c("Emma", "Persuasion"), names_to = "book", values_to = "proportion")

# Wait, the pivot_wider above makes columns "Emma" and "Persuasion".
# Let's redo for a cleaner X-Y plot:
library(scales) # for log scales

freq_wide <- books %>%
    unnest_tokens(word, text) %>%
    filter(str_detect(word, "[a-z]+")) %>%
    count(book, word) %>%
    group_by(book) %>%
    mutate(proportion = n / sum(n)) %>%
    select(-n) %>%
    pivot_wider(names_from = book, values_from = proportion)

# 3. Visualize
# Words near the diagonal line are used equally.
# Words far from the line are distinctive.
ggplot(freq_wide, aes(x = Emma, y = Persuasion)) +
    geom_abline(color = "gray40", lty = 2) +
    geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
    geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
    scale_x_log10(labels = percent_format()) +
    scale_y_log10(labels = percent_format()) +
    labs(title = "Word Frequency Comparison", x = "Emma", y = "Persuasion")
```

### R Exercise 4.2: Network Analysis (Entities & Relationships)

### R Exercise 4.2: Network Analysis (Entities & Relationships)

Named Entity Recognition (NER) is great for finding specific locations or people, but R shines at **Network Analysis**â€”finding how words relate to each other.

We will visualize which words often appear together in Jane Austen's books.

```{r}
#| label: r-network
#| message: false
#| warning: false
library(igraph)
library(ggraph)
library(tidyr)

# 1. Create Bigram Counts (Relationships)
# We filter for common words to avoid a "hairball" graph
bigram_graph <- austen_books() %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(
        !word1 %in% stop_words$word,
        !word2 %in% stop_words$word
    ) %>%
    drop_na(word1, word2) %>%
    count(word1, word2, sort = TRUE) %>%
    filter(n > 20) %>%
    graph_from_data_frame()

# 2. Visualize the Network
# This shows clusters of related concepts
set.seed(2023)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
    geom_edge_link(aes(edge_alpha = n),
        show.legend = FALSE,
        arrow = a, end_cap = circle(.07, "inches")
    ) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void() +
    labs(title = "Common Bigram Network in Jane Austen")
```

---

## Final Challenge: Sentiment Analysis with `bing`

**Task:**
1.  Load the `bing` sentiment lexicon (`get_sentiments("bing")`).
2.  Take **Emma** from `austen_books()`.
3.  **Task:** Count how many "positive" vs "negative" words are in the book.
4.  **Discussion:** Does the book have more positive or negative vocabulary?

*Check your work against the Solution Key.*
