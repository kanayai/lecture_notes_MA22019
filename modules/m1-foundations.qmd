---
title: "Module 1: Foundations & Tokenization"
---

## Theory (30 Minutes)

### Core Concepts: The Lego Analogy

Think of a document (a book, a tweet, a report) like a **completed Lego castle**.


*   **Text Data Analysis** is the process of smashing that castle apart to see what bricks were used.
*   **Tokenization** is the act of taking the wall apart brick-by-brick.
*   **Tokens** are the individual bricks (words).
*   **Corpus** is the giant tub of bricks from all the castles combined.

We analyze the *frequency* and *co-occurrence* of these bricks to infer what kind of structure gave rise to them. (e.g., "This pile has lots of grey 1x2 bricks, it must have been a castle fortress.")

### Two Philosophies: Tidy vs. Matrix

There are two major ways to organize these bricks in code within the R ecosystem.

| Feature | **Tidy Text (tidytext)** | **Document-Term Matrix (tm/topicmodels)** |
| :--- | :--- | :--- |
| **Data Structure** | A long table (Data Frame). One row per token. | A sparse matrix. Rows are documents, columns are words. |
| **Mental Model** | "Spreadsheet" logic. We filter, group, and join. | Mathematical logic. Linear algebra operations. |
| **Primary Libraries**| `tidytext`, `dplyr`, `ggplot2` | `tm`, `topicmodels`, `quanteda` |
| **Best For...** | Exploratory Data Analysis (EDA), Visualization. | Statistical modeling (LDA, TF-IDF scaling). |

### The Details: Text Normalization

One crucial step in tokenization is decision-making about **Normalization**.

*   **Case Sensitivity**: Should "Apple" (the company) and "apple" (the fruit) be counted as the same token?
    *   *Standard approach*: Lowercase everything to group "The" and "the".
    *   *Risk*: You lose the distinction between proper nouns (US) and pronouns (us).
*   **Punctuation**: Usually removed, but crucial for Sentiment Analysis (e.g., "!!!" implies intensity).

---

## Practical (120 Minutes)

### R Exercise 1.1: Tidy Corpus Creation

In this exercise, we will take "messy" book data and convert it into a tidy format.

**Key Functions:**

*   `unnest_tokens(tbl, output, input)`: Splitting a column of text (`input`) into tokens (`output`), flattening the table so there is one token per row.

```{r}
#| label: r-tokenization
#| message: false
library(janeaustenr) # Provides the corpus
library(tidytext) # Provides unnest_tokens()
library(dplyr) # Data manipulation
```

```{r}
# 1. Load data
# austen_books() returns a tibble with a 'text' column (lines of the books)
original_books <- austen_books()

# 2. Convert to tidy format: one-token-per-row
tidy_books <- original_books %>%
    # Grouping allows us to track line numbers relative to each book
    group_by(book) %>%
    mutate(linenumber = row_number()) %>%
    ungroup() %>%
    # The Magic Step: Tokenize the 'text' column into a new 'word' column
    # This defaults to converting to lowercase and stripping punctuation
    unnest_tokens(word, text)

# 3. Inspect the structure
# Notice how the number of rows has exploded? That's characteristic of tidy text.
head(tidy_books)

# 4. Save for use in other modules
# RMD projects do not have this problem
saveRDS(tidy_books, here::here("data", "tidy_books.rds"))
# We use saveRDS to preserve R-specific data types and structure, and
# here::here() to ensure file paths are relative to the project root.
```

### R Exercise 1.2: N-grams and Context

While single words (unigrams) are useful, sometimes we need more context. "New York" means something different than "New" and "York" separately. We can capture this with **N-grams**.

**Key Arguments:**

*   `token = "ngrams"`: Tells `unnest_tokens` to create n-grams instead of words.
*   `n = 2`: Specifies the size of the n-gram (2 for bigrams, 3 for trigrams).

```{r}
#| label: r-ngrams
# 1. Generate Bigrams
austen_bigrams <- original_books %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2)

# 2. Inspect the most common bigrams
# Notice many are "stop words" like "of the", "to be"
austen_bigrams %>%
    count(bigram, sort = TRUE) %>%
    head(10)

# 3. Filtering Bigrams
# We can use separate() to look at the individual words in a bigram
library(tidyr)

bigrams_separated <- austen_bigrams %>%
    separate(bigram, c("word1", "word2"), sep = " ")

# Filter out cases where either word is a stop word
data("stop_words")

bigrams_filtered <- bigrams_separated %>%
    filter(!word1 %in% stop_words$word) %>%
    filter(!word2 %in% stop_words$word)

# 4. View the most interesting bigrams
bigrams_united <- bigrams_filtered %>%
    unite(bigram, word1, word2, sep = " ")

bigrams_united %>%
    count(bigram, sort = TRUE) %>%
    head(10)
```

---

## Key Takeaways

*   **Unstructured -> Structured**: The first step of any analysis is converting raw text strings into a format the computer can count and measure.
*   **Granularity**: We usually start at the *word* level (unigrams), but can move to *n-grams* to capture phrase-level meaning.
*   **Tidy Principles**: Keeping data in a long format (one token per row) allows us to use the entire `tidyverse` toolkit for analysis.

---

## Homework Challenge: The Time Machine

**Objective:** Apply tokenization techniques to a new text.

1.  **Corpus Creation:** Use `gutenbergr` to download H.G. Wells' *The Time Machine* (Gutenberg ID **35**). Convert the text into a tidy format using `unnest_tokens`.
2.  **Filter Question:** Find the top 10 most common **trigrams** (3-word sequences) in the book.
3.  **Advanced:** Try to remove trigrams that contain stop words.

*Check your work against the Solution Key.*